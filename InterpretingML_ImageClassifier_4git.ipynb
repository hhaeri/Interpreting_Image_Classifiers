{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhaeri/Interpreting_Image_Classifiers/blob/main/InterpretingML_ImageClassifier_4git.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BWQ2MzN6WYe"
      },
      "source": [
        "# Interpreting Image ClassifiersðŸ“·"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's unravel the mysteries behind machine learning algorithms with leveraging power of model agnostic explainers (SHAP, LIME, Integrated Gradient)"
      ],
      "metadata": {
        "id": "9R2KW2e0gIKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Description\n"
      ],
      "metadata": {
        "id": "e0VxLjMIlJwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given a pre-trained ResNet model that is trained on Imagenet 1k dataset. Your task is to interpret \"Why the ResNet model detects cars?\"\n",
        "\n",
        "For interpreting a classification task, there are multiple dimensions to choose from (Global vs Local, Model agnostic vs. specific, Inherent vs. post hoc). I will be using a Model agnostic post hoc method and deploy it at a local scale\n",
        "\n",
        "Specifically, I will use LIME, SHAP, and integrated-gradient in this project. For each of these algorithms, I will be documenting the compute time and visualizing their explanations. At the end of the project, I'll be comparing the three evaluation approaches and assessing which I agree with most. So let's dive in! ðŸ’ª\n",
        "\n",
        "<center><img src='https://media.tenor.com/khe_nqmAFJMAAAAC/driverless-car-veritasium.gif'></center>"
      ],
      "metadata": {
        "id": "OWqyx_kHi_IJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMc1B4o1urim"
      },
      "source": [
        "## Setup ðŸ› ï¸\n",
        "Before we start our mission, lets get some gear set up. Firstly, lets install the missing packages and import the necessary libraries.\n",
        "\n",
        "Note: You may have to restart the runtime after installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1dFx3pWqiXs"
      },
      "source": [
        "### Installation of Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPVrmYECxSfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7caVWltC6hUv"
      },
      "outputs": [],
      "source": [
        "# Note: You may have to restart the runtime after installation\n",
        "!pip install ipython\n",
        "!pip install omnixai\n",
        "!pip install dash\n",
        "!pip install dash-bootstrap-components\n",
        "!pip install streamlit\n",
        "## For local tunnel to a proxy server\n",
        "!npm install localtunnel\n",
        "!pip install jupyter-dash\n",
        "#!npm install -g npm to update!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOmqoiNrqqv7"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will import some usual suspects. We will use Pillow Image library to laod/create images. Finally, let us import our main weapon. Let us use [OmniXAI](https://opensource.salesforce.com/OmniXAI/latest/index.html) (Omni eXplainable AI), a Python library for explainable AI (XAI)."
      ],
      "metadata": {
        "id": "PhOmsFd1ddZ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_ZCAeu_6dYH"
      },
      "outputs": [],
      "source": [
        "## The usual suspects\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "import pickle\n",
        "\n",
        "## To build our classifer\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "\n",
        "## Pillow Library Image function alias PilImage\n",
        "from PIL import Image as PilImage\n",
        "\n",
        "## Omnixai library to build our explainer\n",
        "from omnixai.preprocessing.image import Resize\n",
        "from omnixai.data.image import Image\n",
        "from omnixai.explainers.vision import VisionExplainer\n",
        "from omnixai.visualization.dashboard import Dashboard\n",
        "\n",
        "## Streamllit for dashboard\n",
        "import streamlit as st\n",
        "\n",
        "#### NOTE: You may have to restart the run time because of ipython version conflict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEq2_rZY6qj5"
      },
      "outputs": [],
      "source": [
        "## Before we build our classifier, lets make sure to setup the device.\n",
        "## To run this notbeook via GPU: Edit->Notebook settings ->Hardware accelerator -> GPU\n",
        "## If your GPU is working, device is \"cuda\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIFbVqpukVN"
      },
      "source": [
        "## Image Data and Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1tgSQQE6nsH"
      },
      "outputs": [],
      "source": [
        "## Let's start by loading the image that we want to explain\n",
        "url = \"http://images.cocodataset.org/val2017/000000084170.jpg\"\n",
        "url2 = \"https://drive.google.com/uc?id=1V2yA16JxrPUZR_5qKNns1S5kZkUit7LB&export=download\"\n",
        "download = requests.get(url2, stream=True).raw\n",
        "\n",
        "## TODO: Read the image using Pillow and convert the image into RBG\n",
        "### Hint: Use PilImage to read and convert\n",
        "\n",
        "image = Image(PilImage.open(download).convert('RGB'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Print the image shape and view the image\n",
        "\n",
        "## Print the image shape\n",
        "print(image.shape)\n",
        "\n",
        "# Now, let's view it\n",
        "image.to_pil()\n",
        "# Shh! They are napping..."
      ],
      "metadata": {
        "id": "ddqyZv2mjFYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Lets build our classification model. We will use pre-trained ResNet50 model from PyTorch torchvision models.\n",
        "## Make sure to load the model onto the device for gpu\n",
        "\n",
        "## NOTE: Use `resnet18` in case this is too big for the explainer\n",
        "\n",
        "model = models.resnet34(weights = 'DEFAULT').to(device)"
      ],
      "metadata": {
        "id": "K67JdQNgijpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets get a summary of our model using torchsummary\n",
        "from torchsummary import summary\n",
        "## TODO: Print the model summary\n",
        "### Hint: Use image shape for input_size\n",
        "summary(model, input_size=(3,420,640))"
      ],
      "metadata": {
        "id": "mk2KQ6e5uQHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Did you notice the last layer had 1000 classes. Lets import all the classes.\n",
        "## We will later pass this to our explainer\n",
        "classes_url = 'https://gist.githubusercontent.com/DaniFojo/dad37f5bf00ddeb56ed36daf561dbf69/raw/bd006b86300a5886ac7f897a44b0525b75a4b5a1/imagenet_labels.json'\n",
        "imagenet_classes = json.loads(requests.get(classes_url).text)\n",
        "idx2label =  {int(k):v for k,v in imagenet_classes.items()}\n",
        "\n",
        "first_label = idx2label[next(iter(idx2label))]\n",
        "print(f\"The first class label from the ImageNet dataset is: '{first_label}'\")"
      ],
      "metadata": {
        "id": "hLTbu2GHLdT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(idx2label)\n",
        "for i in range(10):\n",
        "  print(idx2label[next(it)])"
      ],
      "metadata": {
        "id": "Sebrm0Hy4Yy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmyv-bo3u9c8"
      },
      "source": [
        "## Buiding our Explainer\n",
        "\n",
        "To build our Explainer for our model, we will use [Vision Explainer](https://opensource.salesforce.com/OmniXAI/v1.2.3/omnixai.explainers.vision.html) by OmniXAI. The explainer needs some pre-processing and post-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnrmP6R1txc0"
      },
      "source": [
        "### Pre-processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGpN6fIttv1O"
      },
      "outputs": [],
      "source": [
        "## TODO: Build the pre-processor pipeline for the explainer\n",
        "\n",
        "# The preprocessing function should convert the image to a Tensor\n",
        "# and then Normalise it\n",
        "\n",
        "#1. Compose the transformations\n",
        "transform = transforms.Compose([\n",
        "    # 1a. write code to resize the image to 256\n",
        "    transforms.Resize(256),\n",
        "    # 1b. write code to center crop 224\n",
        "    transforms.CenterCrop(224),\n",
        "    # 1c. write code to convert the image to tensor\n",
        "    transforms.ToTensor(),\n",
        "    # 1d. write code to normalize the image\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Create the preprocess logic using the transformation built in previous cell\n",
        "### Hint: Use torch.stack and load the images to the device\n",
        "\n",
        "def preprocess(images):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    images: Sequence of images to preprocess using the composed\n",
        "            transformations created above\n",
        "\n",
        "  Returns:\n",
        "    preprocessed_images: Sequence of preprocessed images\n",
        "  \"\"\"\n",
        "  #preprocessed_images = ...\n",
        "  preprocessed_images = torch.stack([transform(image.to_pil()) for image in images]).to(device)\n",
        "  return preprocessed_images\n"
      ],
      "metadata": {
        "id": "zoVmrOM1vdEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVriqWcuVSy"
      },
      "source": [
        "### Post-processor\n",
        "\n",
        "Next, we need to define our post-processing function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea4FZctmvow4"
      },
      "outputs": [],
      "source": [
        "## TODO: Build the post-processor function for the explainer\n",
        "# We will apply a softmax function to the logits obtained in the last layer\n",
        "# in order to convert the prediction scores to probabilities\n",
        "\n",
        "def postprocess(logits):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    logits: Logits from the last layer of the model\n",
        "\n",
        "  Returns:\n",
        "    postprocessed_outputs: Output from the Softmax layer applied to the logits\n",
        "  \"\"\"\n",
        "  postprocessed_outputs = torch.nn.functional.softmax(logits,dim=1)\n",
        "  return postprocessed_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Z03RZ2vzVv"
      },
      "source": [
        "### Vision Explainer\n",
        "Now, construct the explainer using the VisionExplainer class. You'll want to provide it a list of the three explainer types you'd like to try: LIME, SHAP, and integrated gradient. Be sure to check the documentation for the appropriate arguments! See the sample code for VisionExplainer [here](https://opensource.salesforce.com/OmniXAI/v1.2.3/tutorials/vision.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R8gBAgJ6xK8"
      },
      "outputs": [],
      "source": [
        "from omnixai.explainers.vision.agnostic.shap import shap\n",
        "import lime\n",
        "#TODO: Build the VisionExplainer by filling in the blanks\n",
        "explainer = VisionExplainer(\n",
        "    explainers=[\"lime\",\"shap\",\"ig\"],\n",
        "    mode=\"classification\",\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    postprocess=postprocess,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuRVgBBCwvZX"
      },
      "source": [
        "Now, we can generate some explanations for each of the explainers using the explainer.explain() method. This may take couple of minutes on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFR55IOvw5aT"
      },
      "outputs": [],
      "source": [
        "## Time to generate the explanations\n",
        "image_np = Image(data=np.concatenate([image.to_numpy()]),batched=True)\n",
        "local_explanations = explainer.explain(image_np)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets write the local_explantions to a pickle file. We will use this in our dashboard\n",
        "with open('file.pkl', 'wb') as file:\n",
        "    # A new file will be created\n",
        "    pickle.dump(local_explanations, file)"
      ],
      "metadata": {
        "id": "33JVHA0COHCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL66926z7TA1"
      },
      "source": [
        "## Dashboard ðŸ–¼ï¸\n",
        "Now let's create a Dashboard to visualize our different explainers that we just built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKdAir1y7hXD"
      },
      "outputs": [],
      "source": [
        "# Launch a dashboard for visualization using streamlit or gradio\n",
        "\n",
        "## TODO: Fill in the Dashboard parameters\n",
        "\n",
        "dashboard = Dashboard(\n",
        "    instances=image,\n",
        "    local_explanations=local_explanations,\n",
        "    class_names= idx2label\n",
        ")\n",
        "#!nohup npx localtunnel --port 8005 > output.log &\n",
        "#dashboard.show()#port = 8050)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Alternatively, you can view the notebook on browser via the generated link below\n",
        "## Google Colab hosts the server on remote local. Therefore, localhost on your machine will not lead you to the dashboard\n",
        "\n",
        "!nohup npx localtunnel --port 8005 > output.log &\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8005)\"))\n",
        "\n",
        "# If a link does not appear here, open `output.log` from files and use the link to get redirected.\n",
        "# <NOTE> : It might take a minute for the log file to show up. Hit refresh if need be."
      ],
      "metadata": {
        "id": "MJhl4wR77IxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC_0U5yY7jVk"
      },
      "source": [
        "## Take Home Notes:\n",
        "\n",
        "\n",
        "1.   **My thoughts on Explainable AI**: Interpretable AI sounds like a very useful tool to have in my toolbox specially when I am working with machine learning and deep learning models that inherently are not interperatble and the end user can not understand why the model made specific decisions. Interperatble AI is very important for ensuring transparancy, accountablity and trust in AI models. It can be used to address ethical concerns and assists in regulatory compliance. It also can be used to debug and improve AI models.   \n",
        "2.   **Which method do I agree with most?**: In this study I used three different explainers: SHAP, LIME, and Integrated Gradients. SHAP, LIME, and Integrated Gradients offer different approaches to explaining image classifier models. All these three explainers are model agnostic however IG is beter suited for deep learning models. Leveraging the visual capabilities of the OmniXAI I was able to compare the score maps of these three explainares. Based on my observation LIME and IG are easier to interpret and understand by an end user as the depicted influential pixels clearly can be related to the shape of a pickup truck/car. However, it is hard to arrive at the same conclusion using the shap's score map, as it seems that shap is more faithful to the original model and score the whole image area quite similarly (the green dots are spread all around the score map/image). This make the shap a less interpretable explainer as the user will have no clue why the model made the final decision. Between the three explainers tested in this experience I would vote against the SHAP explainer.\n",
        "\n",
        "1.   **IS RESNET doing a good job on detecting the cars?** Yes, the RESNET architecture is able to make sound predictions for the car images that we test in the experince. Using the explainers I can see that the model is able to identify important features of the images that correspond to a car such as windshield, side mirros, fender, etc. The model is also able to adequately box and locate the actual car inside the big image.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computation Times\n",
        "Now let's document the computation time for each explainer: LIME, SHAP, and integrated-gradient."
      ],
      "metadata": {
        "id": "IALK072KVawY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets use hugging face cats vs dogs dataset\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "T2zBHxZv__wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now we will load 5 cat images from the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "## Feel free to change this number. In order to not run out of RAM we use 5 images\n",
        "NUM_IMAGES = 5\n",
        "dataset = load_dataset(\"keremberke/license-plate-object-detection\", 'mini')\n",
        "cars_data = dataset['validation'][0:NUM_IMAGES]['image']\n",
        "cars_data"
      ],
      "metadata": {
        "id": "ZQXzI_tpjVeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Notice that the image sizes are different.\n",
        "## TODO: Convert them to same size using transforms.Resize\n",
        "\n",
        "transform_resize = transforms.Compose([\n",
        "            # 1a. write code to resize the image to 256\n",
        "            transforms.Resize(256),\n",
        "            # 1b. write code to center crop 224\n",
        "            transforms.CenterCrop(224),\n",
        "            # 1c. write code to convert the image to tensor\n",
        "            transforms.ToTensor(),\n",
        "            # 1d. write code to normalize the image\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            transforms.ToPILImage()\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "ooQTgt7lRa_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets use the transformer and stack the images\n",
        "# TODO: Use `transform_resize` and `np.stack`\n",
        "\n",
        "cars = np.stack([transform_resize(car) for car in cars_data])"
      ],
      "metadata": {
        "id": "p8aaCZLuRufe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We will use this explainer function to create independant explainer\n",
        "def explainer(explainer):\n",
        "  return VisionExplainer(\n",
        "    explainers=[explainer],\n",
        "    mode=\"classification\",\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    postprocess=postprocess,\n",
        "  )"
      ],
      "metadata": {
        "id": "WpcEMXkq_yGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: Initialize the explainer for 'Lime', 'SHAP', and 'integrated gradient'\n",
        "lime = explainer('lime')\n",
        "shap = explainer('shap')\n",
        "ig = explainer('ig')"
      ],
      "metadata": {
        "id": "xOxjBo3Zlpj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let us time the results. We will use built-in magic commands in jupyter\n",
        "%time lime_results = lime.explain(cars)"
      ],
      "metadata": {
        "id": "I2NN4qhXY8oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time shap_results = shap.explain(cars)"
      ],
      "metadata": {
        "id": "GbXgRgrimWED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time ig_results = ig.explain(cars)"
      ],
      "metadata": {
        "id": "RXj-xYLQmZ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Google Colab hosts the server on remote local. Therefore, localhost on your machine will not lead you to the dashboard\n",
        "\n",
        "## Open `output.log` from files and use the link to get redirected.\n",
        "## <NOTE> : It might take a minute for the log file to show up. Hit refresh if need be.\n",
        "!nohup npx localtunnel --port 8000 > output.log &"
      ],
      "metadata": {
        "id": "ml_kvswYkOgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Combine all results\n",
        "combine_results = lime_results\n",
        "combine_results['shap'] = shap_results['shap']\n",
        "combine_results['ig'] = ig_results['ig']\n",
        "\n",
        "## Lets visualize the results on the Dashboard\n",
        "dashboard = Dashboard(\n",
        "    instances=Image(cars,batched =True),\n",
        "    local_explanations=combine_results,\n",
        "    class_names=idx2label\n",
        ")\n",
        "## Do not change the port\n",
        "## <NOTE> Once you open the link, it might take a minute or two for the website to load fully. Be patient :)\n",
        "dashboard.show(port=8000)"
      ],
      "metadata": {
        "id": "AsEFIWoCK3KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final ThoughtsðŸŽ‰"
      ],
      "metadata": {
        "id": "yz1qeEXWH2Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP can be computationally expensive, especially for deep learning models, as it requires evaluating the model's predictions for a large number of subsets of features. LIME is computationally less intensive compared to SHAP and is suitable for quick, local model interpretations. Integrated Gradients can be computationally efficient for deep learning models, especially when using fast approximations. This explains the different runtimes of these three explainers (49.3 sec, 1 min 57 sec and 6.98 sec for lime, shap and ig respectively).\n",
        "\n",
        "Based on the run time observation as well as the interpretability discussion I made in the previous section I dont recommend use of SHAP explainer for this experience. Both LIME and IG can provide computationally effiecent and interpretable explaianation although the IG explainer is 7 times faster and might scale up much easier\n"
      ],
      "metadata": {
        "id": "Vp0yrizOPyqO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TO3ndeTGQia0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}