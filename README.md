# Interpreting_Image_Classifiers
Interpreting Image Classifiers using Vision Explainer by OmniXAI

## What is Interpretable AI?
As industries in a variety of fields, from health care to robotics to commerce become more reliant on Machine Learning models, and especially deep learning models, understanding why models reach their conclusions has become incredibly important. For policymakers, researchers, and regulators who may be making decisions based on these models, understanding the factors that influence a model's conclusion is positively essential. That's where interpretability comes in!

To ensure we understand why our models make the decisions they do, we can add an interpretability step between the train and evaluate phases! Of course, since the ML development process is iterative, we won't just be considering interpretability once. Instead, we'll be revisiting it throughout the development process.

<img width="528" alt="image" src="https://github.com/hhaeri/Interpreting_Image_Classifiers/assets/91407046/01d82db3-f9b5-4bf8-a687-1b6ca491be15">

## Overcoming The Challenges of ML with the use of Interpretable AI?

Interpretability makes it possible to assess our ML models for factors like bias and fairness, robustness, privacy, causality, and trustworthiness. For example, interpretability can be essential for ensuring the safety and well-being of the people impacted by the model. For example, ML is often used in high-stakes settings where poor performance can lead to harmful outcomes. In healthcare, practitioners need to understand a model's performance to ensure proper care for patients. When it comes to autonomous driving, we don't want the vehicle to hit a parked car, or worse yet, a pedestrian.

Interpretability also plays a vital role in rooting out bias in algorithms. One recent example of this was the identification of facial and gender bias in the Amazon Recognition Commercial System. In the dataset, 67% of the people cooking were women, but the algorithm predicted that to be 84%! Biases like these can have real-world ramifications – In the financial sector, for example, bias can impact who is approved for credit or loans.

Without interpretability, we can't trust our models – We won't know what our model will predict in extreme cases, and we'll be poorly equipped to identify specific shortcomings or biases. We'll also find it difficult to identify adversarial examples. These are examples that have been engineered to cause our model to fail. By incorporating interpretability into our model, we can see whether it makes similar mistakes as humans, and we can be better equipped to update the model when things go wrong.
